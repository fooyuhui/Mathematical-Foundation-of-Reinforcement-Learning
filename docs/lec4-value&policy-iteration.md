# Value Iteration and Policy Iteration Algorithms

## Value iteration algorithm

How to solve the Bellman optimality equation?

$$
v=f(v)=\max_\pi(r_\pi+\gamma P_\pi v)
$$

$$
v_{k+1} = f(v_k) = \max_\pi(r_\pi + P_\pi v_k) \quad k=1,2,3\dots
$$

where $v_0$ can be arbitrary

This algorithm is called *value iteration*

The algorithm

$$
v_{k+1} = f(v_k) = \max_\pi(r_\pi + P_\pi v_k) \quad k=1,2,3\dots
$$

can be decomposed to two steps.

* Step 1: **policy update**. This step is to solve

  $$
  \pi_{k+1} = \argmax_\pi(r_\pi+\gamma P_\pi v_k)
  $$

  where $v_k$ is given.

* Step 2: **value update**.
  
  $$
  v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k
  $$

Next, we need to study the elementwise form in order to implement the algorithm.

Step 1: **Policy update**

The elementwise form of

$$
\pi_{k+1}=\argmax_\pi(r_\pi+ \gamma P_\pi v_k)
$$

is

$$
\pi_{k+1}=\argmax_\pi\sum_a\pi(a|s)\underbrace{\left(\sum_r p(r|s,a)r + \gamma\sum_{s'}p(s'|s,a)v_k(s') \right)}_{q_k(s,a)} \quad s\in \mathcal{S}
$$

The optimal policy solving the above optimization problem is

$$
\pi_{k+1}(a|s)=\begin{cases}
    1 & a = a_k^*(s) \\
    0 & a \ne a_k^*(s)
\end{cases}
$$

where $a_k^*(s)=\argmax_a q_k(s,a)$ . $\pi_{k+1}$ is called a **greedy policy** , since it simple select the greatest q-value.

Step 2: **value update**.
  
The elementwise form of

$$
v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k
$$

is

$$
v_{k+1}(s)=\sum_a\pi_{k+1}(a|s)\underbrace{\left(\sum_r p(r|s,a)r + \gamma\sum_{s'}p(s'|s,a)v_k(s') \right)}_{q_k(s,a)} \quad s\in \mathcal{S}
$$

Since $\pi_{k+1}$ is greedy, the above equation is simply

$$
v_{k+1}(s) = \max_a q_k(a,s)
$$

## Policy iteration algorithm

Given a random initial policy $\pi_0$

* Step 1: policy evaluation (PE)

  This step is to calculate the state value of $\pi_k$ :

  $$
  v_{\pi_k}=r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}
  $$

  Note that $v_{\pi_k}$ is a state value function

* Step 2: policy improvement (PI)

  $$
  \pi_{k+1} = \argmax_\pi(r_\pi + \gamma P_\pi v_{\pi_k})
  $$

This algorithm leads to a sequence

$$
\pi_0\xrightarrow{PE}v_{\pi_0}\xrightarrow{PI}\pi_1\xrightarrow{PE}v_{\pi_1}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\cdots
$$

---
How to get the state value $v_{\pi_k}$ by solving the Bellman equation?

$$
v_{\pi_k}=r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}
$$

* closed-form solution
* iterative solution

---

In the policy improvement step, why is the new policy $\pi_{k+1}$ better than $\pi_k$ ?

If $\pi_{k+1}=\argmax_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$ , then $v_{\pi_{k+1}}\ge v_{\pi_k}$ for any $k$

---

Why can such an iterative algorithm finally reach an optimal policy?

> Theorem (Convergence of Policy Iteration)
>
> The state value sequence $\{v_{\pi_k}\}_{k=0}^\infty$ generated by the policy iteration algorithem converges to the optimal state value $v^*$ . As a result, the policy sequence $\{\pi_k\}_{k=0}^\infty$ converges to an optimal policy.

---

Step 1: Policy evaluation

* Matrix-vector form: $v_{\pi_k}^{(j+1)} = r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}^{(j)} \quad j=0,1,2,\dots$
* Elementwise form:
  
  $$
  v_{\pi_k}^{(j+1)}(s)=\sum_a\pi_k(a|s)\left(\sum_r p(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v_{\pi_k}^{(j)}(s') \right) \quad s\in \mathcal{S}
  $$

  Stop when $j\to\infty$ or $j$ is sufficiently large or $\|v_{\pi_k}^{(j+1)}-v_{\pi_k}^{(j)}\|$ is sufficiently small.

Step 2: Policy improvement

* Matrix-vector form: $\pi_{k+1} = \argmax_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$
* Elementwise form:
  
  $$
  \pi_{k+1}(s) = \argmax_\pi\sum_a\pi(a|s)\underbrace{\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_{\pi_k}(s')}_{q_{\pi_k}(s,a)} \quad s\in\mathcal{S}
  $$

  Here, $q_{\pi_k}(s,a)$ is the action value under policy $\pi_k$ . Let
  
  $$
  a_k^*(s)=\argmax_a q_{\pi_k}(a,s)
  $$

  Then, the greedy policy is

  $$
  \pi_{k+1}(a|s)\begin{cases}
    1 & a = a_k^*(s) \\
    0 & a \ne a_k^*(s)
  \end{cases}
  $$

## Truncated policy iteration algorithm

Policy iteration: start from $\pi_0$

Value iteration: start from $v_0$

Consider the step of solving $v_{\pi_1}=r_{\pi_1}+\gamma v_{\pi_1}$

$$
\begin{align}
\nonumber & v_{\pi_1}^{(0)}=v_0 \\
\nonumber value\ iteration \leftarrow v_1\leftarrow &v_{\pi_1}^{(1)}=r_{\pi_1}+\gamma P_{\pi_1}v_{\pi_1}^{(0)} \\
\nonumber &v_{\pi_1}^{(2)}=r_{\pi_1}+\gamma P_{\pi_1}v_{\pi_1}^{(1)} \\
\nonumber &\cdots \\
\nonumber truncated\ policy\ iteration\leftarrow \bar{v}_1\leftarrow &v_{\pi_1}^{(j)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(j-1)} \\
\nonumber &\cdots \\
\nonumber policy\ iteration \leftarrow v_{\pi_1}\leftarrow &v_{\pi_1}^{(\infty)} = r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(\infty)} \\
\end{align}
$$
